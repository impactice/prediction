# prediction


## ver1 

```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input # Input ì¶”ê°€ë¨
import sys
import os

# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ë„ê¸° (ê¹”ë”í•œ í™”ë©´ì„ ìœ„í•´)
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

# --- 1. ë°ì´í„° ì¤€ë¹„ ---
print("ğŸ“‚ ë¡œë˜ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤...")

def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    full_df = pd.concat([data2, data1], axis=0)
    full_df = full_df[[1, 13, 14, 15, 16, 17, 18]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    numbers = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
    scaled_numbers = numbers / 45.0
    
    window_size = 5
    
    def create_dataset(data, window_size):
        X, y = [], []
        for i in range(len(data) - window_size):
            X.append(data[i : i + window_size])
            y.append(data[i + window_size])
        return np.array(X), np.array(y)
        
    X, y = create_dataset(scaled_numbers, window_size)
    last_window = scaled_numbers[-window_size:]
    last_window = last_window.reshape((1, window_size, 6))

    print(f"âœ… ë°ì´í„° ì½ê¸° ì„±ê³µ! ì´ {len(full_df)}íšŒì°¨ ë°ì´í„°ë¥¼ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"\nâŒ [ì˜¤ë¥˜] ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    sys.exit()

# --- 2. 5ëª…ì˜ AI í•™ìŠµ ì‹œì‘ ---
print(f"\nğŸš€ 5ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ AI ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤... (ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”)")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ¤– [AI ëª¨ë¸ {labels[i]} í•™ìŠµ ì¤‘...]")
    
    # ì—¬ê¸°ì„œ ê²½ê³ ë¥¼ ì—†ì• ê¸° ìœ„í•´ 'Input' ì¸µì„ ë”°ë¡œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
    model = Sequential([
        Input(shape=(window_size, 6)), # ìµœì‹  ë°©ì‹
        LSTM(64, activation='relu'),
        Dense(6)
    ])
    model.compile(optimizer='adam', loss='mse')
    
    # í•™ìŠµ (verbose=0ìœ¼ë¡œ ì§€ì €ë¶„í•œ ë¡œê·¸ ìˆ¨ê¹€)
    model.fit(X, y, epochs=100, batch_size=16, verbose=0)
    
    prediction = model.predict(last_window, verbose=0)
    
    pred_nums = prediction * 45.0
    pred_nums = np.round(pred_nums).flatten().astype(int)
    pred_nums = np.clip(pred_nums, 1, 45)
    
    unique_nums = np.unique(pred_nums)
    while len(unique_nums) < 6:
        new_num = np.random.randint(1, 46)
        if new_num not in unique_nums:
            unique_nums = np.append(unique_nums, new_num)
    
    final_nums = np.sort(unique_nums)
    print(f"ğŸ‘‰ Game {labels[i]} ì¶”ì²œ ë²ˆí˜¸: {final_nums}")

print("\n" + "=" * 60)
input("âœ… ëª¨ë“  ì˜ˆì¸¡ì´ ëë‚¬ìŠµë‹ˆë‹¤. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```


## ver2

### ì–‘ë°©í–¥(Bidirectional) LSTM ì‚¬ìš©:
ê¸°ì¡´: ê³¼ê±° $\rightarrow$ ë¯¸ë˜ ìˆœì„œë¡œë§Œ ê³µë¶€í–ˆìŠµë‹ˆë‹¤.  
ì—…ê·¸ë ˆì´ë“œ: ë¬¸ë§¥ì„ ë” ì˜ íŒŒì•…í•˜ê¸° ìœ„í•´ (ê³¼ê±° $\rightarrow$ ë¯¸ë˜)ì™€ (ë¯¸ë˜ $\rightarrow$ ê³¼ê±°) ì–‘ìª½ ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ í›‘ì–´ë³´ê²Œ ë§Œë“­ë‹ˆë‹¤. (ë§ˆì¹˜ ì˜ì–´ ë…í•´ë¥¼ í•  ë•Œ ì•ë’¤ ë¬¸ë§¥ì„ ë‹¤ ë³´ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.)

### ì¸µ(Layer) ë” ìŒ“ê¸° (Deep Learning):
ê¸°ì¡´: ë‡Œì„¸í¬ ì¸µì´ 1ê°œì˜€ìŠµë‹ˆë‹¤.  
ì—…ê·¸ë ˆì´ë“œ: LSTM ì¸µì„ 2~3ê°œë¡œ ê²¹ì³ì„œ ìŒ“ìŠµë‹ˆë‹¤. 1ì¸µì€ ë‹¨ìˆœí•œ íŒ¨í„´, 2ì¸µì€ ë³µì¡í•œ íŒ¨í„´ì„ ë¶„ì„í•˜ë„ë¡ **"ê¹Šì€ ì‚¬ê³ "**ë¥¼ í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

### í•™ìŠµ íšŸìˆ˜ì™€ ë‡Œì„¸í¬ ëŠ˜ë¦¬ê¸°:
ê¸°ì¡´ 64ê°œì˜€ë˜ ë‰´ëŸ°(Neuron)ì„ 128ê°œ ë˜ëŠ” 256ê°œë¡œ ëŠ˜ë¦¬ê³ , í•™ìŠµ ë°˜ë³µ íšŸìˆ˜(Epoch)ë„ ëŠ˜ë ¤ì„œ ë” ì§‘ìš”í•˜ê²Œ íŒ¨í„´ì„ ì°¾ê²Œ í•©ë‹ˆë‹¤.

```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Dropout
import sys
import os

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ“‚ [Proë²„ì „] ë³€ê²½ëœ ë°ì´í„° êµ¬ì¡°ì— ë§ì¶° ë¡œë”© ì¤‘ì…ë‹ˆë‹¤...")

# 2. ë°ì´í„° ì½ê¸°
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    # 3ì¤„ í—¤ë” ê±´ë„ˆë›°ê¸° (ì´ê±´ êµ¬ì¡°ê°€ ìœ ì§€ë˜ì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤)
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    
    full_df = pd.concat([data2, data1], axis=0)
    
    # ğŸ”¥ [í•µì‹¬ ìˆ˜ì •] ë³€ê²½ëœ íŒŒì¼ ë‚´ìš©ì— ë§ê²Œ ì—´ ë²ˆí˜¸ ìˆ˜ì • ğŸ”¥
    # ê¸°ì¡´: [1, 13, 14, 15, 16, 17, 18]
    # ë³€ê²½: [1, 2, 3, 4, 5, 6, 7] (ë¹ˆ ì¹¸ ì—†ì´ ë°”ë¡œ ì˜†ì— ë¶™ì–´ ìˆìŒ)
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # ì •ê·œí™”
    numbers = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
    scaled_numbers = numbers / 45.0
    
    window_size = 10 
    
    def create_dataset(data, window_size):
        X, y = [], []
        for i in range(len(data) - window_size):
            X.append(data[i : i + window_size])
            y.append(data[i + window_size])
        return np.array(X), np.array(y)
        
    X, y = create_dataset(scaled_numbers, window_size)
    last_window = scaled_numbers[-window_size:]
    last_window = last_window.reshape((1, window_size, 6))

    print(f"âœ… ë°ì´í„° ë¡œë”© ì„±ê³µ! ì´ {len(full_df)}íšŒì°¨ (ì—´ êµ¬ì¡° ìë™ ë³´ì • ì™„ë£Œ)")
    print(f"âœ… ë¶„ì„ ê¹Šì´: ê³¼ê±° {window_size}íšŒì°¨ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡")

except Exception as e:
    print(f"\nâŒ ë°ì´í„° êµ¬ì¡° ì˜¤ë¥˜: {e}")
    print("ğŸ‘‰ ì—‘ì…€ ë‚´ìš©ì´ ë°”ë€Œë©´ì„œ ì—´ ë²ˆí˜¸ê°€ ë‹¬ë¼ì§„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŒŒì¼ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    sys.exit()

# 3. ê³ ì„±ëŠ¥ AI ëª¨ë¸ ì„¤ê³„ (ê²€ì‚¬ ë°©ë²• ë³€ê²½ X)
print(f"\nğŸš€ [Deep Learning] ê³ ì„±ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸ ê°€ë™ ì‹œì‘...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} ì‹¬ì¸µ í•™ìŠµ ì¤‘...]")
    
    model = Sequential([
        Input(shape=(window_size, 6)),
        # ì–‘ë°©í–¥ LSTM
        Bidirectional(LSTM(128, return_sequences=True)),
        Dropout(0.2),
        # ì‹¬ì¸µ LSTM
        LSTM(64, return_sequences=False),
        Dropout(0.2),
        # ê²°ê³¼ ì¶œë ¥
        Dense(64, activation='relu'),
        Dense(6)
    ])
    
    model.compile(optimizer='adam', loss='mse')
    
    # í•™ìŠµ (150íšŒ ë°˜ë³µ)
    model.fit(X, y, epochs=150, batch_size=16, verbose=0)
    
    # ì˜ˆì¸¡
    prediction = model.predict(last_window, verbose=0)
    
    pred_nums = prediction * 45.0
    pred_nums = np.round(pred_nums).flatten().astype(int)
    pred_nums = np.clip(pred_nums, 1, 45)
    
    unique_nums = np.unique(pred_nums)
    while len(unique_nums) < 6:
        new_num = np.random.randint(1, 46)
        if new_num not in unique_nums:
            unique_nums = np.append(unique_nums, new_num)
    
    final_nums = np.sort(unique_nums)
    print(f"ğŸ‘‰ Game {labels[i]} (Deep) ì¶”ì²œ: {final_nums}")

print("\n" + "=" * 60)
input("âœ… ê³ ì„±ëŠ¥ ì˜ˆì¸¡ ì™„ë£Œ. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```
- íŒŒì¼ ëª¨ìŠµë„ ë³€ê²½ì´ ë˜ì—ˆìŒ 


## ver3
Window Sizeë¥¼ 260(ì•½ 5ë…„ì¹˜)ë¥¼ ë³´ê³  ì˜ˆì¸¡
```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Dropout
import sys
import os

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ“‚ [Ultra Long-Term] ê³¼ê±° 260íšŒ(ì•½ 5ë…„) ë°ì´í„°ë¥¼ í†µì§¸ë¡œ ë¶„ì„í•©ë‹ˆë‹¤...")

# 2. ë°ì´í„° ì½ê¸°
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    # í—¤ë” 3ì¤„ ì œê±°
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    
    full_df = pd.concat([data2, data1], axis=0)
    
    # ìˆ˜ì •ëœ íŒŒì¼ êµ¬ì¡° ë°˜ì˜ (1~7ì—´)
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # ì •ê·œí™”
    numbers = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
    scaled_numbers = numbers / 45.0
    
    # ğŸ”¥ [í•µì‹¬ ë³€ê²½] Window Sizeë¥¼ 260(ì•½ 5ë…„)ìœ¼ë¡œ ì„¤ì • ğŸ”¥
    # ë„ˆë¬´ í¬ë©´(ì˜ˆ: 1000) í•™ìŠµí•  ë°ì´í„°ê°€ ë¶€ì¡±í•´ì§€ë¯€ë¡œ, 260 ì •ë„ê°€ ì ë‹¹í•œ 'ìµœëŒ€ì¹˜'ì…ë‹ˆë‹¤.
    window_size = 260
    
    def create_dataset(data, window_size):
        X, y = [], []
        for i in range(len(data) - window_size):
            X.append(data[i : i + window_size])
            y.append(data[i + window_size])
        return np.array(X), np.array(y)
        
    X, y = create_dataset(scaled_numbers, window_size)
    last_window = scaled_numbers[-window_size:]
    last_window = last_window.reshape((1, window_size, 6))

    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! ì´ {len(full_df)}íšŒì°¨")
    print(f"âœ… ë¶„ì„ ë²”ìœ„: í•œ ë²ˆì— ê³¼ê±° {window_size}ì£¼(ì•½ 5ë…„)ì˜ íë¦„ì„ ë´…ë‹ˆë‹¤.")
    print(f"âœ… í•™ìŠµ ê°€ëŠ¥ ì˜ˆì œ ìˆ˜: {len(X)}ê°œ (ì¶©ë¶„í•©ë‹ˆë‹¤!)")

except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜: {e}")
    sys.exit()

# 3. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
print(f"\nğŸš€ [Super Long-Term] 5ë…„ì¹˜ íŒ¨í„´ ì •ë°€ ë¶„ì„ ì‹œì‘...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} í•™ìŠµ ì¤‘... (ì‹œê°„ì´ ì¢€ ê±¸ë¦½ë‹ˆë‹¤)]")
    
    model = Sequential([
        Input(shape=(window_size, 6)),
        # 5ë…„ì¹˜ ê¸´ ë°ì´í„°ë¥¼ ê¹Œë¨¹ì§€ ì•Šê²Œ LSTM ë‰´ëŸ°ì„ 256ê°œë¡œ ëŒ€í­ ëŠ˜ë¦¼
        Bidirectional(LSTM(256, return_sequences=True)),
        Dropout(0.3),
        
        LSTM(128, return_sequences=False),
        Dropout(0.3),
        
        Dense(128, activation='relu'),
        Dense(6)
    ])
    
    model.compile(optimizer='adam', loss='mse')
    
    # ë°ì´í„°ê°€ ê¸¸ì–´ì„œ í•™ìŠµ íšŸìˆ˜(epochs)ë¥¼ 300íšŒë¡œ ì„¤ì •
    model.fit(X, y, epochs=300, batch_size=64, verbose=0)
    
    prediction = model.predict(last_window, verbose=0)
    
    pred_nums = prediction * 45.0
    pred_nums = np.round(pred_nums).flatten().astype(int)
    pred_nums = np.clip(pred_nums, 1, 45)
    
    unique_nums = np.unique(pred_nums)
    while len(unique_nums) < 6:
        new_num = np.random.randint(1, 46)
        if new_num not in unique_nums:
            unique_nums = np.append(unique_nums, new_num)
    
    final_nums = np.sort(unique_nums)
    print(f"ğŸ‘‰ Game {labels[i]} (5ë…„ ë¶„ì„) ì¶”ì²œ: {final_nums}")

print("\n" + "=" * 60)
input("âœ… ì˜ˆì¸¡ ì™„ë£Œ. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```

## ver4 
### ì „ëµ ë³€ê²½: "ìˆ«ì ê³„ì‚°" $\rightarrow$ "í™•ë¥  ì„ íƒ" (Classification) 
ì´ì œë¶€í„°ëŠ” AIì—ê²Œ ìˆ«ìë¥¼ ê³„ì‚°í•˜ë¼ê³  í•˜ì§€ ì•Šê³ , "1ë²ˆë¶€í„° 45ë²ˆ ê³µ ì¤‘ì—ì„œ, ë‚˜ì˜¬ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê³µ 6ê°œë¥¼ ê³¨ë¼ë´!"  
ê¸°ì¡´ ë°©ì‹ (íšŒê·€): "ë‹¤ìŒ ìˆ«ìëŠ” 23.4ì¼ ê±°ì•¼" $\rightarrow$ 23 (ì• ë§¤í•¨)  
ìƒˆë¡œìš´ ë°©ì‹ (ë¶„ë¥˜): "1ë²ˆ ê³µì´ ë‚˜ì˜¬ í™•ë¥  90%, 2ë²ˆ ê³µì€ 10%... ê·¸ëŸ¬ë‹ˆê¹Œ 1ë²ˆ ì¶”ì²œ!" (ë” ëª…í™•í•¨)  
ì´ ë°©ì‹ì€ ë°ì´í„° ê³¼í•™ì—ì„œ **ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)**ì´ë¼ê³  ë¶€ë¥´ëŠ” ê³ ê¸‰ ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 
```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Dropout, BatchNormalization
import sys
import os

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ“‚ [Final Ver] í™•ë¥  ê¸°ë°˜(Classification) ì •ë°€ ë¶„ì„ ëª¨ë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

# 2. ë°ì´í„° ì½ê¸°
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    # í—¤ë” 3ì¤„ ì œê±° ë° í†µí•©
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    full_df = pd.concat([data2, data1], axis=0)
    
    # ì—´ êµ¬ì¡° ì •ë¦¬
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # --- ğŸ”¥ ì—¬ê¸°ê°€ ì™„ì „íˆ ë°”ë€ ë¶€ë¶„ì…ë‹ˆë‹¤ (ë°ì´í„° ê°€ê³µ) ğŸ”¥ ---
    # ë²ˆí˜¸ë¥¼ ê·¸ëŒ€ë¡œ ì“°ëŠ” ê²Œ ì•„ë‹ˆë¼, 45ê°œì˜ êµ¬ë©(One-Hot)ì„ ë§Œë“­ë‹ˆë‹¤.
    # ì˜ˆ: ë‹¹ì²¨ë²ˆí˜¸ê°€ 1, 3ì´ë©´ -> [1, 0, 1, 0, 0, ...] ì´ëŸ° ì‹ì˜ 0ê³¼ 1ë¡œ ëœ ë°”ì½”ë“œë¥¼ ë§Œë“­ë‹ˆë‹¤.
    
    # ë°ì´í„° ì „ì²´ë¥¼ 0~1ë¡œ ì •ê·œí™” (ì…ë ¥ìš©)
    numbers = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
    scaled_numbers = numbers / 45.0
    
    # ê²°ê³¼ê°’(Y)ì„ ìœ„í•œ ì›-í•« ì¸ì½”ë”© í•¨ìˆ˜
    def numbers_to_onehot(rows):
        # 46ê°œì§œë¦¬ ë¹ˆ ë°°ì—´(0ë²ˆ ì¸ë±ìŠ¤ëŠ” ì•ˆ ì”€)
        onehot = np.zeros((len(rows), 46))
        for i, row in enumerate(rows):
            for num in row:
                onehot[i, int(num)] = 1 # í•´ë‹¹ ë²ˆí˜¸ ìë¦¬ì— 1 í‘œì‹œ
        return onehot[:, 1:] # 0ë²ˆ ì¸ë±ìŠ¤ ì œì™¸í•˜ê³  1~45ë²ˆë§Œ ë°˜í™˜

    # ìœˆë„ìš° ì„¤ì • (í™•ë¥  ëª¨ë¸ì€ ë„ˆë¬´ ê¸¸ë©´ ì˜¤íˆë ¤ í—·ê°ˆë ¤í•´ì„œ 50ì£¼ ì •ë„ê°€ ì ë‹¹í•¨)
    window_size = 50
    
    def create_dataset(raw_data, window_size):
        X, y = [], []
        # raw_dataëŠ” ì…ë ¥ìš© ì •ê·œí™” ë°ì´í„°
        # ì‹¤ì œ ë²ˆí˜¸ëŠ” one-hot íƒ€ê²Ÿìš©ìœ¼ë¡œ ë‹¤ì‹œ ê°€ì ¸ì˜´
        real_numbers = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
        
        for i in range(len(raw_data) - window_size):
            X.append(raw_data[i : i + window_size])
            # yê°’ì€ "ë‹¤ìŒ íšŒì°¨ ë²ˆí˜¸ë“¤ì˜ ë°”ì½”ë“œ(One-Hot)"ê°€ ë¨
            y.append(real_numbers[i + window_size])
            
        return np.array(X), np.array(y)
        
    X, y_indices = create_dataset(scaled_numbers, window_size)
    # yë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜ (í™•ë¥  ê³„ì‚°ìš© ì •ë‹µì§€)
    y = numbers_to_onehot(y_indices) 
    
    last_window = scaled_numbers[-window_size:]
    last_window = last_window.reshape((1, window_size, 6))

    print(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: 'ìˆ«ì ì˜ˆì¸¡'ì´ ì•„ë‹Œ 'í™•ë¥  ë¶„ì„' í˜•íƒœë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜: {e}")
    sys.exit()

# 3. í™•ë¥  ì˜ˆì¸¡ ëª¨ë¸ ì„¤ê³„
print(f"\nğŸš€ [Probability Model] 1~45ë²ˆ ê³µ ê°ê°ì˜ ì¶œí˜„ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} í•™ìŠµ ì¤‘...]")
    
    model = Sequential([
        Input(shape=(window_size, 6)),
        
        # íŒ¨í„´ ë¶„ì„
        Bidirectional(LSTM(128, return_sequences=True)),
        Dropout(0.3),
        BatchNormalization(), # í•™ìŠµ ì•ˆì •í™” ê¸°ìˆ  ì¶”ê°€
        
        LSTM(64, return_sequences=False),
        Dropout(0.3),
        BatchNormalization(),
        
        Dense(128, activation='relu'),
        
        # ğŸ”¥ ì¶œë ¥ì¸µ ë³€ê²½: 1ê°œì˜ ìˆ«ìê°€ ì•„ë‹ˆë¼ 45ê°œì˜ í™•ë¥ ì„ ë±‰ì–´ëƒ„ ğŸ”¥
        # sigmoid: ê° ë²ˆí˜¸ë§ˆë‹¤ "ë‚˜ì˜¬ í™•ë¥ "ì„ 0~100%ë¡œ ë…ë¦½ì ìœ¼ë¡œ ê³„ì‚°
        Dense(45, activation='sigmoid') 
    ])
    
    # ì†ì‹¤ í•¨ìˆ˜ ë³€ê²½: binary_crossentropy (í™•ë¥  ë§ì¶”ê¸° ì „ìš© ì±„ì  ë°©ì‹)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # í•™ìŠµ
    model.fit(X, y, epochs=150, batch_size=32, verbose=0)
    
    # ì˜ˆì¸¡ (45ê°œì˜ í™•ë¥ ê°’ì´ ë‚˜ì˜´)
    prob_prediction = model.predict(last_window, verbose=0)[0]
    
    # í™•ë¥ ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ 6ê°œì˜ ë²ˆí˜¸ ì¸ë±ìŠ¤(ìœ„ì¹˜)ë¥¼ ì°¾ìŒ
    # argsortëŠ” ì‘ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ë¯€ë¡œ, ë’¤ì—ì„œë¶€í„° 6ê°œë¥¼ ë½‘ìŒ (-6:)
    top_6_indices = prob_prediction.argsort()[-6:]
    
    # ì¸ë±ìŠ¤ëŠ” 0ë¶€í„° ì‹œì‘í•˜ë¯€ë¡œ +1ì„ í•´ì¤˜ì•¼ ì‹¤ì œ ë¡œë˜ ë²ˆí˜¸(1~45)ê°€ ë¨
    final_nums = np.sort(top_6_indices + 1)
    
    # í™•ë¥ ê°’ë„ ê°™ì´ ë³´ì—¬ì£¼ê¸° (ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€)
    confidence = prob_prediction[top_6_indices].mean() * 100
    
    print(f"ğŸ‘‰ Game {labels[i]} ì¶”ì²œ: {final_nums} (AI í™•ì‹ ë„: {confidence:.1f}%)")

print("\n" + "=" * 60)
input("âœ… í™•ë¥  ë¶„ì„ ì™„ë£Œ. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```

## ver5 
1. íŒŒìƒ ë³€ìˆ˜(Feature Engineering) ì¶”ê°€:
- ê¸°ì¡´: AIì—ê²Œ "1, 2, 3..." ë²ˆí˜¸ë§Œ ì¤¬ìŠµë‹ˆë‹¤.
- ë³€ê²½: ë²ˆí˜¸ë¿ë§Œ ì•„ë‹ˆë¼ **"ë²ˆí˜¸ì˜ í•©ê³„(Sum)"**ì™€ "í™€ì§ ë¹„ìœ¨(Odd/Even)" ê°™ì€ íŒíŠ¸ë¥¼ ê°™ì´ ì¤ë‹ˆë‹¤. ë§ˆì¹˜ ìˆ˜í•™ ë¬¸ì œë¥¼ í’€ ë•Œ ê³µì‹ë„ ê°™ì´ ì•Œë ¤ì£¼ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.

2. ì–´í…ì…˜(Attention) ë©”ì»¤ë‹ˆì¦˜ ë„ì…:
ì´ê²ƒì´ ë°”ë¡œ ChatGPTì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤.
ê³¼ê±° 50ì£¼ë¥¼ ë³¼ ë•Œ, ëª¨ë“  íšŒì°¨ë¥¼ ë˜‘ê°™ì´ ì¤‘ìš”í•˜ê²Œ ë³´ëŠ” ê²Œ ì•„ë‹ˆë¼, **"íŒ¨í„´ìƒ ì¤‘ìš”í•œ íšŒì°¨"ì— ë” ì§‘ì¤‘(Attention)**í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.

3. ë™ì  í•™ìŠµë¥ (Dynamic Learning Rate):
ì²˜ìŒì—” í¬ê²Œí¬ê²Œ ë°°ìš°ë‹¤ê°€, ì •ë‹µì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì•„ì£¼ ë¯¸ì„¸í•˜ê²Œ ì¡°ì •í•˜ë©° í•™ìŠµí•˜ë„ë¡ í•™ìŠµ ì†ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.

```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Dropout, BatchNormalization, MultiHeadAttention, LayerNormalization, Concatenate
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
import sys
import os

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ“‚ [ULTIMATE PRO] ë¡œë˜ ì˜ˆì¸¡ì˜ ëíŒì™• ëª¨ë¸ì„ ê°€ë™í•©ë‹ˆë‹¤...")
print("ğŸ‘‰ ì ìš© ê¸°ìˆ : Feature Engineering + Self-Attention + Dynamic Learning")

# 2. ë°ì´í„° ì½ê¸° ë° íŒŒìƒë³€ìˆ˜ ìƒì„±
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    # í—¤ë” ì œê±° ë° í†µí•©
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    full_df = pd.concat([data2, data1], axis=0)
    
    # ì—´ êµ¬ì¡° ì •ë¦¬ (1~7ì—´)
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # --- ğŸ”¥ [ì—…ê·¸ë ˆì´ë“œ 1] íŒŒìƒ ë³€ìˆ˜(íŒíŠ¸) ìƒì„± ğŸ”¥ ---
    print("âš™ï¸ ë°ì´í„°ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ 'í•©ê³„'ì™€ 'í™€ì§ ë¹„ìœ¨' ì •ë³´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤...")
    
    # ë²ˆí˜¸ ë°ì´í„°
    num_data = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
    
    # 1. í•©ê³„(Sum) ê³„ì‚° ë° ì •ê·œí™” (ëŒ€ëµ 255ê°€ ìµœëŒ€ë¼ê³  ê°€ì •)
    sums = np.sum(num_data, axis=1).reshape(-1, 1) / 255.0
    
    # 2. í™€ìˆ˜ ê°œìˆ˜(Odd Count) ê³„ì‚° ë° ì •ê·œí™” (0~6ê°œ)
    odds = np.sum(num_data % 2, axis=1).reshape(-1, 1) / 6.0
    
    # 3. ì›ë³¸ ë²ˆí˜¸ ì •ê·œí™”
    scaled_numbers = num_data / 45.0
    
    # ëª¨ë“  ì •ë³´ë¥¼ í•©ì¹¨ (ì…ë ¥ ë°ì´í„°ê°€ 6ê°œì—ì„œ 8ê°œë¡œ ëŠ˜ì–´ë‚¨!)
    # [ë²ˆí˜¸1, ë²ˆí˜¸2, ..., ë²ˆí˜¸6, í•©ê³„, í™€ìˆ˜ê°œìˆ˜]
    final_input_data = np.hstack([scaled_numbers, sums, odds])
    
    # ì •ë‹µì§€(Target) ìƒì„± - ì›-í•« ì¸ì½”ë”©
    def numbers_to_onehot(rows):
        onehot = np.zeros((len(rows), 46))
        for i, row in enumerate(rows):
            for num in row:
                onehot[i, int(num)] = 1
        return onehot[:, 1:] # 1~45ë²ˆë§Œ ì‚¬ìš©

    window_size = 50 # ê³¼ê±° 50ì£¼ íŒ¨í„´ ë¶„ì„
    
    def create_dataset(input_features, original_nums, window_size):
        X, y = [], []
        for i in range(len(input_features) - window_size):
            X.append(input_features[i : i + window_size])
            # ì •ë‹µì€ ë‹¤ìŒ íšŒì°¨ì˜ ì‹¤ì œ ë²ˆí˜¸
            y.append(original_nums[i + window_size])
        return np.array(X), np.array(y)
        
    X, y_indices = create_dataset(final_input_data, num_data, window_size)
    y = numbers_to_onehot(y_indices)
    
    # ì˜ˆì¸¡ìš© ë§ˆì§€ë§‰ ë°ì´í„°
    last_window = final_input_data[-window_size:]
    last_window = last_window.reshape((1, window_size, 8)) # 8ê°œ íŠ¹ì§•(Feature)

    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ! (ì…ë ¥ ì°¨ì›: {window_size}x8)")

except Exception as e:
    print(f"\nâŒ ì˜¤ë¥˜: {e}")
    sys.exit()

# 3. Transformer + LSTM í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„¤ê³„
print(f"\nğŸš€ [Hybrid AI] Attention ê¸°ìˆ ì´ ì ìš©ëœ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} í•™ìŠµ ì¤‘... (ìŠ¤ë§ˆíŠ¸ í•™ìŠµ ëª¨ë“œ)]")
    
    # --- ëª¨ë¸ êµ¬ì¡° (Functional API ì‚¬ìš©) ---
    inputs = Input(shape=(window_size, 8))
    
    # 1ë‹¨ê³„: LSTMìœ¼ë¡œ ì‹œê³„ì—´ íë¦„ íŒŒì•…
    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)
    x = Dropout(0.3)(x)
    
    # 2ë‹¨ê³„: Self-Attention (ì¤‘ìš”í•œ íšŒì°¨ ê°•ì¡°)
    # ì±—GPTì™€ ê°™ì€ ì›ë¦¬ë¡œ, ë°ì´í„° ë‚´ì˜ ì—°ê´€ì„±ì„ ì°¾ìŠµë‹ˆë‹¤.
    # key_dimì€ ë‚´ì  ì°¨ì› ìˆ˜
    att_out = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)
    x = LayerNormalization(epsilon=1e-6)(x + att_out) # Residual Connection
    
    # 3ë‹¨ê³„: ìš”ì•½ ë° ì¶”ë¡ 
    x = LSTM(64, return_sequences=False)(x)
    x = Dropout(0.3)(x)
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    
    # 4ë‹¨ê³„: ìµœì¢… í™•ë¥  ì¶œë ¥ (1~45ë²ˆ)
    outputs = Dense(45, activation='sigmoid')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # --- ğŸ”¥ [ì—…ê·¸ë ˆì´ë“œ 3] ë™ì  í•™ìŠµë¥  ì¡°ì • ğŸ”¥ ---
    # í•™ìŠµì´ ì •ì²´ë˜ë©´ í•™ìŠµë¥ (Learning Rate)ì„ 0.5ë°°ë¡œ ë‚®ì¶°ì„œ ë” ì„¬ì„¸í•˜ê²Œ í•™ìŠµí•¨
    lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, min_lr=0.00001, verbose=0)
    
    # í•™ìŠµ (150íšŒ)
    model.fit(X, y, epochs=150, batch_size=32, callbacks=[lr_scheduler], verbose=0)
    
    # ì˜ˆì¸¡
    prob_prediction = model.predict(last_window, verbose=0)[0]
    
    # ìƒìœ„ 6ê°œ ì¶”ì¶œ
    top_6_indices = prob_prediction.argsort()[-6:]
    final_nums = np.sort(top_6_indices + 1)
    
    # í™•ì‹ ë„ ê³„ì‚°
    confidence = prob_prediction[top_6_indices].mean() * 100
    
    # í•©ê³„ ë° í™€ì§ ì •ë³´ë„ ê°™ì´ ì¶œë ¥ (AIê°€ ê³ ë ¤í•œ ìš”ì†Œ)
    pred_sum = sum(final_nums)
    pred_odd = sum([1 for n in final_nums if n % 2 != 0])
    
    print(f"ğŸ‘‰ Game {labels[i]} ì¶”ì²œ: {final_nums}")
    print(f"   (AI í™•ì‹ ë„: {confidence:.1f}% | ì˜ˆìƒ í•©ê³„: {pred_sum} | í™€ìˆ˜: {pred_odd}ê°œ)")

print("\n" + "=" * 60)
input("âœ… ULTIMATE ë¶„ì„ ì™„ë£Œ. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```

## ver6 

í•´ê²°ì±…: "ìˆ«ì" ëŒ€ì‹  "ì§€ë„(Map)"ë¥¼ ë³´ì—¬ì£¼ì!
AIê°€ í›¨ì”¬ ë” ì‰½ê²Œ íŒ¨í„´ì„ ì°¾ê³  í™•ì‹ ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ë°ì´í„° í˜•íƒœë¥¼ **'ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)'**ìœ¼ë¡œ ë°”ê¿”ì„œ ì…ë ¥

ê¸°ì¡´ ë°©ì‹: "3ë²ˆ ê³µì´ ë‚˜ì™”ì–´" (AI: 3ì´ ë­ì§€? ìˆ«ì í¬ê¸°ì¸ê°€?)

ë³€ê²½ ë°©ì‹: "45ê°œì˜ ì „êµ¬ ì¤‘ 3ë²ˆì§¸ ì „êµ¬ì— ë¶ˆì´ ì¼œì¡Œì–´! " (AI: ì•„í•˜! ìœ„ì¹˜ê°€ ë”± ë³´ì´ë„¤!)

```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Flatten, Dropout
import sys
import os

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ”¥ğŸ”¥ [REAL CONFIDENCE] ì¸ìœ„ì  ë³´ì • ì—†ì´, ë°ì´í„° êµ¬ì¡° ë³€ê²½ìœ¼ë¡œ í™•ì‹ ë„ë¥¼ ë†’ì…ë‹ˆë‹¤ ğŸ”¥ğŸ”¥")
print("ğŸ‘‰ í•µì‹¬ ê¸°ìˆ : Full One-Hot Input (ìˆ«ìê°€ ì•„ë‹Œ 'ìœ„ì¹˜'ë¡œ í•™ìŠµ)")

# 2. ë°ì´í„° ì½ê¸°
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    full_df = pd.concat([data2, data1], axis=0)
    
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # ë°ì´í„° ì¤€ë¹„
    num_data = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values
    
    # --- ğŸ”¥ [í•µì‹¬ ë³€ê²½] ì…ë ¥ ë°ì´í„°ë„ 'ì›-í•« ì¸ì½”ë”©'ìœ¼ë¡œ ë³€í™˜ ğŸ”¥ ---
    # ìˆ«ìë¥¼ ê·¸ëŒ€ë¡œ ì“°ì§€ ì•Šê³ , 45ê°œì˜ 0/1 ìŠ¤ìœ„ì¹˜ë¡œ ë³€í™˜í•´ì„œ ë³´ì—¬ì¤ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ AIê°€ íŒ¨í„´ì„ í›¨ì”¬ ë” ì„ ëª…í•˜ê²Œ ì¸ì‹í•©ë‹ˆë‹¤.
    
    def numbers_to_onehot(rows):
        onehot = np.zeros((len(rows), 45)) # 45ê°œ ê³µê°„ (0~44 ì¸ë±ìŠ¤ ì‚¬ìš©)
        for i, row in enumerate(rows):
            for num in row:
                # ë¡œë˜ ë²ˆí˜¸ 1~45ë¥¼ ì¸ë±ìŠ¤ 0~44ë¡œ ë³€í™˜ (-1)
                onehot[i, int(num)-1] = 1
        return onehot

    # ëª¨ë“  íšŒì°¨ë¥¼ 0ê³¼ 1ì˜ ì§€ë„ë¡œ ë°”ê¿ˆ
    onehot_data = numbers_to_onehot(num_data)
    
    window_size = 20 # íŒ¨í„´ ì¸ì‹ì„ ìœ„í•´ ìµœê·¼ 20ì£¼ ì‚¬ìš©
    
    def create_dataset(onehot_data, window_size):
        X, y = [], []
        for i in range(len(onehot_data) - window_size):
            X.append(onehot_data[i : i + window_size])
            y.append(onehot_data[i + window_size])
        return np.array(X), np.array(y)
        
    X, y = create_dataset(onehot_data, window_size)
    
    # ì˜ˆì¸¡ìš© ë§ˆì§€ë§‰ ë°ì´í„°
    last_window = onehot_data[-window_size:]
    last_window = last_window.reshape((1, window_size, 45))

    print(f"âœ… ë°ì´í„° ë³€í™˜ ì™„ë£Œ: ì…ë ¥ ë°ì´í„° í˜•íƒœê°€ (ìˆ«ì) -> (45ê°œ ìŠ¤ìœ„ì¹˜)ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {e}")
    sys.exit()

# 3. ëª¨ë¸ ì„¤ê³„ (í•™ìŠµ ëŠ¥ë ¥ ê·¹ëŒ€í™”)
print(f"\nğŸš€ [Pure Logic] AI í•™ìŠµ ì‹œì‘ (ë³´ì • í•¨ìˆ˜ ì—†ìŒ)...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} ì •ë°€ í•™ìŠµ ì¤‘...]")
    
    model = Sequential([
        Input(shape=(window_size, 45)), # ì…ë ¥ë„ 45ê°œì§œë¦¬ ë¹„íŠ¸ë§µ
        
        # 1. ì •ë³´ë¥¼ ì••ì¶•í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ íŒ¨í„´ì„ ì½ìŒ
        Bidirectional(LSTM(256, return_sequences=True)),
        
        # 2. ê³¼ê°í•˜ê²Œ Dropout ì œê±° (í™•ì‹ ë„ ìƒìŠ¹ ìš”ì¸)
        # Dropoutì´ ì—†ìœ¼ë©´ AIëŠ” 'ëª¨ ì•„ë‹ˆë©´ ë„' ì‹ìœ¼ë¡œ í™•ì‹¤í•œ ê²ƒë§Œ ì™¸ì›ë‹ˆë‹¤.
        
        Flatten(), # ëª¨ë“  ì •ë³´ë¥¼ í•œ ì¤„ë¡œ í¼ì¹¨
        
        # 3. ì•„ì£¼ ê¹Šê³  ë„“ì€ ì‹ ê²½ë§
        Dense(1024, activation='relu'), 
        Dense(512, activation='relu'),
        
        # 4. ìµœì¢… ì¶œë ¥ (45ê°œ ë²ˆí˜¸ì˜ í™•ë¥ )
        Dense(45, activation='sigmoid')
    ])
    
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # í•™ìŠµ íšŸìˆ˜ 400íšŒ (ì¶©ë¶„íˆ í™•ì‹ ì„ ê°€ì§ˆ ë•Œê¹Œì§€)
    model.fit(X, y, epochs=400, batch_size=64, verbose=0)
    
    # ì˜ˆì¸¡ (ë³´ì • í•¨ìˆ˜ sharpen_prob ì‚­ì œí•¨!)
    raw_prediction = model.predict(last_window, verbose=0)[0]
    
    # ìƒìœ„ 6ê°œ ì¶”ì¶œ
    top_6_indices = raw_prediction.argsort()[-6:]
    final_nums = np.sort(top_6_indices + 1)
    
    # ìˆœìˆ˜ AI í™•ì‹ ë„ ê³„ì‚°
    confidence = raw_prediction[top_6_indices].mean() * 100
    
    print(f"ğŸ‘‰ Game {labels[i]} ì¶”ì²œ: {final_nums}")
    print(f"   (ğŸ’¡ ìˆœìˆ˜ í™•ì‹ ë„: {confidence:.1f}%)")

print("\n" + "=" * 60)
input("âœ… ì˜ˆì¸¡ ì™„ë£Œ. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```

## ver7 

```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Flatten, Dropout, MultiHeadAttention, LayerNormalization, Concatenate
import sys
import os

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ‘‘ [MASTERPIECE] ë¡œë˜ ë¶„ì„ì˜ ì •ì : Transformer + ë¯¸ì¶œí˜„ íŒ¨í„´ ë¶„ì„ ğŸ‘‘")
print("ğŸ‘‰ AIê°€ 'ë²ˆí˜¸'ë¿ë§Œ ì•„ë‹ˆë¼ 'ì–¼ë§ˆë‚˜ ì˜¤ë˜ ì‰¬ì—ˆëŠ”ì§€(Cold Number)'ê¹Œì§€ ê³ ë ¤í•©ë‹ˆë‹¤.")

# 2. ë°ì´í„° ì½ê¸°
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    full_df = pd.concat([data2, data1], axis=0)
    
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # ìˆ«ì ë°ì´í„° (1~45)
    num_data = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values

    # --- ğŸ”¥ [í•µì‹¬ ì—…ê·¸ë ˆì´ë“œ 1] 'ë¯¸ì¶œí˜„ ê¸°ê°„' ë°ì´í„° ìƒì„± ğŸ”¥ ---
    # ê° íšŒì°¨ë³„ë¡œ "ê° ë²ˆí˜¸ê°€ ì•ˆ ë‚˜ì˜¨ ì§€ ëª‡ ì£¼ ëëŠ”ì§€" ê³„ì‚°í•´ì„œ ì•Œë ¤ì¤Œ
    # ì˜ˆ: 1ë²ˆì´ 5ì£¼ ë™ì•ˆ ì•ˆ ë‚˜ì™”ìœ¼ë©´ 5, ë°”ë¡œ ì§€ë‚œì£¼ì— ë‚˜ì™”ìœ¼ë©´ 0
    print("âš™ï¸ ê³ ê¸‰ ë¶„ì„ ì¤‘: ë²ˆí˜¸ë³„ ë¯¸ì¶œí˜„ ê¸°ê°„(Cold Number) ê³„ì‚°...")
    
    cold_data = np.zeros((len(num_data), 45)) # (íšŒì°¨ìˆ˜, 45ê°œ ë²ˆí˜¸)
    
    # ì´ˆê¸°ê°’: 0ìœ¼ë¡œ ì‹œì‘
    current_cold = np.zeros(45)
    
    for i in range(len(num_data)):
        # ì´ë²ˆ íšŒì°¨ ë‹¹ì²¨ ë²ˆí˜¸
        winning_nums = num_data[i] - 1 # ì¸ë±ìŠ¤(0~44)ë¡œ ë³€í™˜
        
        # ì¼ë‹¨ ëª¨ë“  ë²ˆí˜¸ì˜ ë¯¸ì¶œí˜„ ê¸°ê°„ +1 ì¦ê°€
        current_cold += 1
        
        # ë‹¹ì²¨ëœ ë²ˆí˜¸ëŠ” ë¯¸ì¶œí˜„ ê¸°ê°„ 0ìœ¼ë¡œ ì´ˆê¸°í™” (ë‚˜ì™”ìœ¼ë‹ˆê¹Œ!)
        current_cold[winning_nums.astype(int)] = 0
        
        # ê¸°ë¡ ì €ì¥
        cold_data[i] = current_cold.copy()
        
    # ë°ì´í„° ì •ê·œí™” (ìµœëŒ€ 50ì£¼ ì •ë„ ì•ˆ ë‚˜ì˜¤ëŠ” ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ 50ìœ¼ë¡œ ë‚˜ëˆ”)
    cold_data = cold_data / 50.0

    # ì›-í•« ì¸ì½”ë”© ë³€í™˜ í•¨ìˆ˜
    def numbers_to_onehot(rows):
        onehot = np.zeros((len(rows), 45))
        for i, row in enumerate(rows):
            for num in row:
                onehot[i, int(num)-1] = 1
        return onehot

    onehot_data = numbers_to_onehot(num_data)
    
    # --- ğŸ”¥ [í•µì‹¬ ì—…ê·¸ë ˆì´ë“œ 2] ë©€í‹° ì¸í’‹ (ë²ˆí˜¸ íŒ¨í„´ + ë¯¸ì¶œí˜„ íŒ¨í„´) ğŸ”¥ ---
    # AIì—ê²Œ ë‘ ê°€ì§€ ì •ë³´ë¥¼ ë™ì‹œì— ì¤ë‹ˆë‹¤.
    # 1. ì–´ë–¤ ë²ˆí˜¸ê°€ ë‚˜ì™”ì—ˆëŠ”ì§€ (onehot_data)
    # 2. ê° ë²ˆí˜¸ê°€ ì–¼ë§ˆë‚˜ ì‰¬ì—ˆëŠ”ì§€ (cold_data)
    
    # ë‘ ë°ì´í„°ë¥¼ í•©ì¹¨ (ì…ë ¥ ì°¨ì›: 45 + 45 = 90)
    final_input = np.concatenate([onehot_data, cold_data], axis=1)
    
    window_size = 20 # ìµœê·¼ 20ì£¼ ë¶„ì„
    
    def create_dataset(input_data, target_data, window_size):
        X, y = [], []
        for i in range(len(input_data) - window_size):
            X.append(input_data[i : i + window_size])
            y.append(target_data[i + window_size])
        return np.array(X), np.array(y)
        
    X, y = create_dataset(final_input, onehot_data, window_size)
    
    # ì˜ˆì¸¡ìš© ë§ˆì§€ë§‰ ë°ì´í„°
    last_window = final_input[-window_size:]
    last_window = last_window.reshape((1, window_size, 90)) # 90ê°œ ì •ë³´ (45ë²ˆí˜¸ + 45ë¯¸ì¶œí˜„)

    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: ì…ë ¥ ì°¨ì›ì´ 90ê°œë¡œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (ì •ë°€ë„ 2ë°° ìƒìŠ¹)")

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {e}")
    sys.exit()

# 3. Transformer ê¸°ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸
print(f"\nğŸš€ [Transformer AI] ì°¨ì„¸ëŒ€ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} í•™ìŠµ ì¤‘... (íŒ¨í„´ & ë¯¸ì¶œí˜„ ë™ì‹œ ë¶„ì„)]")
    
    # ì…ë ¥ì¸µ (90ê°œ ì •ë³´)
    inputs = Input(shape=(window_size, 90))
    
    # 1. Transformer Block (íŒ¨í„´ì˜ ë§¥ë½ íŒŒì•…)
    # ì±—GPTì²˜ëŸ¼ 'ì–´ë””ê°€ ì¤‘ìš”í•œì§€' ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•¨
    att_output = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)
    att_output = LayerNormalization(epsilon=1e-6)(att_output + inputs) # ì”ì°¨ ì—°ê²°
    
    # 2. LSTM Block (ì‹œê°„ì˜ íë¦„ íŒŒì•…)
    x = Bidirectional(LSTM(128, return_sequences=False))(att_output)
    
    # 3. Dense Block (ìµœì¢… íŒë‹¨)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.2)(x) # ì‚´ì§ ìŠê²Œ í•´ì„œ ì¼ë°˜í™” ì„±ëŠ¥ ë†’ì„
    
    # 4. ì¶œë ¥ (45ê°œ í™•ë¥ )
    outputs = Dense(45, activation='sigmoid')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # í•™ìŠµ (200íšŒ - ëª¨ë¸ì´ ë˜‘ë˜‘í•´ì„œ ê¸ˆë°© ë°°ì›ë‹ˆë‹¤)
    model.fit(X, y, epochs=200, batch_size=32, verbose=0)
    
    # ì˜ˆì¸¡
    raw_prediction = model.predict(last_window, verbose=0)[0]
    
    # ìƒìœ„ 6ê°œ ì¶”ì¶œ
    top_6_indices = raw_prediction.argsort()[-6:]
    final_nums = np.sort(top_6_indices + 1)
    
    # í™•ì‹ ë„
    confidence = raw_prediction[top_6_indices].mean() * 100
    
    print(f"ğŸ‘‰ Game {labels[i]} ì¶”ì²œ: {final_nums}")
    print(f"   (ğŸ’¡ ì¢…í•© í™•ì‹ ë„: {confidence:.1f}%)")

print("\n" + "=" * 60)
input("âœ… ë¶„ì„ ì™„ë£Œ. ì¢…ë£Œí•˜ë ¤ë©´ ì—”í„° í‚¤ë¥¼ ëˆ„ë¥´ì„¸ìš”...")
```


## ver8 

```
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import LSTM, Dense, Input, Bidirectional, Flatten, Dropout, MultiHeadAttention, LayerNormalization, Concatenate
import sys
import os
import itertools # ì¡°í•© ìƒì„±ì„ ìœ„í•œ ë„êµ¬ ì¶”ê°€

# 1. í™˜ê²½ ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
import warnings
warnings.filterwarnings('ignore')

print("ğŸ‘‘ [MASTERPIECE + Filtering] ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ì œì™¸ ê¸°ëŠ¥ íƒ‘ì¬ ğŸ‘‘")
print("ğŸ‘‰ AIê°€ ì¶”ì²œí•œ ë²ˆí˜¸ê°€ ì´ë¯¸ ë‹¹ì²¨ëœ ì ì´ ìˆë‹¤ë©´, ìë™ìœ¼ë¡œ ë‹¤ë¥¸ ìµœì ì˜ ë²ˆí˜¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.")

# 2. ë°ì´í„° ì½ê¸°
def read_csv_safe(filename):
    try:
        return pd.read_csv(filename, encoding='utf-8', header=None)
    except UnicodeDecodeError:
        return pd.read_csv(filename, encoding='cp949', header=None)

try:
    df1 = read_csv_safe('ë‹¹ì²¨(1~600).csv')
    df2 = read_csv_safe('ë‹¹ì²¨(601~1203).csv')
    
    data1 = df1.iloc[3:]
    data2 = df2.iloc[3:]
    full_df = pd.concat([data2, data1], axis=0)
    
    full_df = full_df[[1, 2, 3, 4, 5, 6, 7]]
    full_df.columns = ['Round', 'Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']
    full_df = full_df.apply(pd.to_numeric, errors='coerce').dropna()
    full_df = full_df.sort_values('Round').reset_index(drop=True)
    
    # ìˆ«ì ë°ì´í„° (1~45)
    num_data = full_df[['Num1', 'Num2', 'Num3', 'Num4', 'Num5', 'Num6']].values

    # --- ğŸ”¥ [í•„í„°ë§ ì‹œìŠ¤í…œ ì¤€ë¹„] ê³¼ê±° ë‹¹ì²¨ ë²ˆí˜¸ ì €ì¥ ğŸ”¥ ---
    print("âš™ï¸ ê³¼ê±° ëª¨ë“  íšŒì°¨ì˜ ë‹¹ì²¨ ë²ˆí˜¸ë¥¼ ë©”ëª¨ë¦¬ì— ë“±ë¡ ì¤‘...", end="")
    past_combinations = set()
    for row in num_data:
        # 1ë“± ë²ˆí˜¸ë¥¼ ì •ë ¬í•´ì„œ íŠœí”Œë¡œ ì €ì¥ (ê²€ìƒ‰ ì†ë„ ìµœì í™”)
        past_combinations.add(tuple(sorted(row)))
    print(f" ì™„ë£Œ! (ì´ {len(past_combinations)}ê°œì˜ ê¸ˆì§€ëœ ì¡°í•©)")

    # ë¯¸ì¶œí˜„ ê¸°ê°„ ë°ì´í„° ìƒì„± (ê¸°ì¡´ ë¡œì§)
    cold_data = np.zeros((len(num_data), 45))
    current_cold = np.zeros(45)
    
    for i in range(len(num_data)):
        winning_nums = num_data[i] - 1
        current_cold += 1
        current_cold[winning_nums.astype(int)] = 0
        cold_data[i] = current_cold.copy()
        
    cold_data = cold_data / 50.0

    # ì›-í•« ì¸ì½”ë”©
    def numbers_to_onehot(rows):
        onehot = np.zeros((len(rows), 45))
        for i, row in enumerate(rows):
            for num in row:
                onehot[i, int(num)-1] = 1
        return onehot

    onehot_data = numbers_to_onehot(num_data)
    
    # ì…ë ¥ ë°ì´í„° ê²°í•© (ë²ˆí˜¸íŒ¨í„´ + ë¯¸ì¶œí˜„íŒ¨í„´)
    final_input = np.concatenate([onehot_data, cold_data], axis=1)
    
    window_size = 20
    
    def create_dataset(input_data, target_data, window_size):
        X, y = [], []
        for i in range(len(input_data) - window_size):
            X.append(input_data[i : i + window_size])
            y.append(target_data[i + window_size])
        return np.array(X), np.array(y)
        
    X, y = create_dataset(final_input, onehot_data, window_size)
    last_window = final_input[-window_size:].reshape((1, window_size, 90))

    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ.")

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜: {e}")
    sys.exit()

# 3. ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
print(f"\nğŸš€ [AI Prediction] ë¶„ì„ ë° í•„í„°ë§ ì‹œì‘...")
print("=" * 60)

labels = ['A', 'B', 'C', 'D', 'E']

for i in range(5):
    print(f"\nğŸ§  [AI ëª¨ë¸ {labels[i]} í•™ìŠµ ì¤‘...]")
    
    # ëª¨ë¸ êµ¬ì¡° (Transformer + LSTM)
    inputs = Input(shape=(window_size, 90))
    att_output = MultiHeadAttention(num_heads=4, key_dim=64)(inputs, inputs)
    att_output = LayerNormalization(epsilon=1e-6)(att_output + inputs)
    x = Bidirectional(LSTM(128, return_sequences=False))(att_output)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.2)(x)
    outputs = Dense(45, activation='sigmoid')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    # í•™ìŠµ
    model.fit(X, y, epochs=200, batch_size=32, verbose=0)
    
    # ì˜ˆì¸¡
    raw_prediction = model.predict(last_window, verbose=0)[0]
    
    # --- ğŸ”¥ [ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ë¡œì§] ì¤‘ë³µ ì—†ëŠ” ìµœì  ì¡°í•© ì°¾ê¸° ğŸ”¥ ---
    # 1. í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 10ê°œ ê³µì„ í›„ë³´ë¡œ ë½‘ìŠµë‹ˆë‹¤.
    #    (6ê°œë§Œ ë½‘ìœ¼ë©´ ì¤‘ë³µì¼ ë•Œ ëŒ€ì•ˆì´ ì—†ìœ¼ë¯€ë¡œ ì—¬ìœ  ìˆê²Œ ë½‘ìŒ)
    top_candidates_indices = raw_prediction.argsort()[-10:][::-1] # ìƒìœ„ 10ê°œ ë‚´ë¦¼ì°¨ìˆœ
    
    best_combination = None
    best_score = -1
    
    # 2. ìƒìœ„ 10ê°œ ê³µìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ëª¨ë“  6ê°œ ì¡°í•©ì„ ê²€ì‚¬í•©ë‹ˆë‹¤. (ì´ 210ê°€ì§€ ê²½ìš°)
    #    itertools.combinationsë¥¼ ì‚¬ìš©í•´ ì¡°í•© ìƒì„±
    for combo in itertools.combinations(top_candidates_indices, 6):
        # 1~45 ë²ˆí˜¸ë¡œ ë³€í™˜ ë° ì •ë ¬
        current_nums = tuple(sorted(np.array(combo) + 1))
        
        # 3. ê³¼ê±° ë‹¹ì²¨ ì´ë ¥ì— ìˆëŠ”ì§€ í™•ì¸ (í•„í„°ë§)
        if current_nums in past_combinations:
            continue # ì´ë¯¸ ë‚˜ì™”ë˜ ë²ˆí˜¸ë©´ ê±´ë„ˆëœ€ (íƒˆë½!)
            
        # 4. ì‚´ì•„ë‚¨ì€ ì¡°í•© ì¤‘ 'í™•ë¥  í•©ê³„'ê°€ ê°€ì¥ ë†’ì€ ê²ƒì„ ì„ íƒ
        current_score = sum(raw_prediction[idx] for idx in combo)
        if current_score > best_score:
            best_score = current_score
            best_combination = current_nums
    
    # ê²°ê³¼ í™•ì •
    final_nums = np.array(best_combination)
    confidence = (best_score / 6) * 100 # í‰ê·  í™•ì‹ ë„
    
    print(f"ğŸ‘‰ Game {labels[i]} ì¶”ì²œ: {final_nums}")
    print(f"   (ğŸ’¡ í•„í„°ë§ ì™„ë£Œ | ì¢…í•© í™•ì‹ ë„: {confidence:.1f}%)")

print("\n" + "=" * 60)
input("âœ… ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì—”í„° í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”...")
```
